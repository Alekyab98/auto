from datetime import datetime
from airflow import DAG
import os
import sys
import yaml
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
)
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from functools import partial
from datetime import datetime, timedelta


BASE_DIR = "/home/airflow/gcs/dags/vz-it-gudv-dtwndo-0"
sys.path.append(f"{BASE_DIR}/nqes_site/python")


#from vz_de_common_observability_integration import publishLog, create_do_dict
from DO_utils import publishLog, create_do_dict

project = os.environ['GCP_PROJECT']
with open(f"{BASE_DIR}/nqes_site/config/base_config.yml", 'r') as file:
    base_config = yaml.full_load(file)

with open(f"{BASE_DIR}/nqes_site/config/gudv_nqes_site.yml", 'r') as file:
    dag_config = yaml.full_load(file)

config_values = {}


filtered_base_dict = dict(filter(lambda elem: elem[0] == project, base_config.items()))
filtered_dict = dict(filter(lambda elem: elem[0] == project, dag_config.items()))

if len(filtered_base_dict) > 0:
    base_value = filtered_base_dict[project][0]
    config_values = {**config_values, **base_value}
else:
    print("No config found exiting...")
    sys.exit(-1)
if len(filtered_dict) > 0:
    app_value = filtered_dict[project][0]
    config_values = {**config_values, **app_value}
else:
    print("No config found exiting...")
    sys.exit(-1)

GCP_PROJECT_ID = config_values['gcp_project']
bq_connection_id = config_values['google_cloud_conn_id']
region = config_values['region']
DAG_ID = config_values['dag_id']
base_directory = config_values['base_directory']
env = config_values['env']
dataset_id = config_values['dataset_id']
stored_proc = config_values['stored_proc']
table_name = config_values['table_name']
schedule_interval = config_values['schedule_interval']
failure_email_alert_distro = config_values['failure_email_alert_distro']

process_date = '{{ data_interval_end.strftime("%Y-%m-%d") }}'
trans_date = '{{ data_interval_end.subtract(days=1).strftime("%Y-%m-%d") }}'


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(year=2025, month=4, day=30, hour=9, minute=00),
    'email': [failure_email_alert_distro],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=3)
}

dag = DAG(
    dag_id=DAG_ID,
    schedule_interval=schedule_interval,
    catchup=True,
    default_args=default_args,
    description='This DAG call Stored Procedure',
    concurrency=int(config_values['concurrency']),
    max_active_runs=int(config_values['max_active_runs']),
    tags=["dtwin","nw_research_assistant","network_genie","nqes_site"]
)


do_dict = create_do_dict(config_values)

start = DummyOperator(task_id='start',
                      dag=dag,
                      on_success_callback=partial(publishLog, "PROGRESS", do_dict),
                      on_failure_callback=partial(publishLog, "FAILURE", do_dict))

#** Source Check Task **
source_check_query = """
    SELECT COUNT(*) AS status_check
    FROM vz-it-pr-fjpv-mlopdo-0.mlops_curated_tbls_v.run_dt_cntrl_tbl_v2
    WHERE process_nm IN ('ETL_NTWK_DMA_NQES_SITE')
    AND status = 'COMPLETE'
    AND run_dt = '{{ trans_date }}'
"""

#This task checks the source status
source_check = BigQueryInsertJobOperator(
    task_id="source_check",
    dag=dag,
    gcp_conn_id=bq_connection_id,
    configuration={
        "query": {
            "query": source_check_query,
            "useLegacySql": False
        }
    },
    on_failure_callback=partial(publishLog, "FAILURE", do_dict)
)

#** Wait & Retry if Source Check Fails **
wait_for_data = DummyOperator(
    task_id='wait_for_data',
    dag=dag,
    trigger_rule=TriggerRule.ONE_FAILED,
    retry_delay=timedelta(hours=3),
    retries=3
)

# # ** Alert if All Retries Fail **
# alert_failure = DummyOperator(
#     task_id='alert_failure',
#     dag=dag,
#     trigger_rule=TriggerRule.ALL_FAILED,
#     on_failure_callback=partial(publishLog, "FAILURE", do_dict)
# )

call_nqes_site_sp = BigQueryInsertJobOperator(
        task_id="call_nqes_site_sp",
        dag=dag,
        gcp_conn_id=bq_connection_id,
        configuration={
                         "query": {
                              "query": f"CALL {dataset_id}.{stored_proc}('{trans_date}','{process_date}')",
                              "useLegacySql": False,
                              }
                         },
        on_failure_callback=partial(publishLog, "FAILURE", do_dict)

)
end = DummyOperator(task_id='end',
                    dag=dag,
                    on_success_callback=partial(publishLog, "SUCCESS", do_dict),
                    on_failure_callback=partial(publishLog, "FAILURE", do_dict))

start >> source_check
source_check >> call_nqes_site_sp >> end
source_check >> wait_for_data >> alert_failure


#start  >> call_nqes_site_sp >> end
